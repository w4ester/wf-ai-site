<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>WF-AI Site</title>
    <link>https://w4ester.github.io/wf-ai-site</link>
    <description>A digital garden for AI experiments, projects, and ideas from the Baltimore AI Producers Lab.</description>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 02:35:17 +0000</lastBuildDate>
    <atom:link href="https://w4ester.github.io/wf-ai-site/feed.xml" rel="self" type="application/rss+xml"/>
    <author>w4ester</author>
    <item>
      <title>Four Years of Local AI: From Experiment to Infrastructure</title>
      <link>https://w4ester.github.io/wf-ai-site</link>
      <description><![CDATA[<h1>Four Years of Local AI: From Experiment to Infrastructure</h1>
<p><em>2026-01-13</em></p>
<p>I ran my first local LLM in 2022. Back then it felt like tinkeringâ€”an expensive curiosity. Now it's the foundation of how I believe we all deserve to access and build AI actions.</p>
<hr />
<h2>The Journey: 2022 to 2026</h2>
<p>When I started running language models locally in 2022, GPT-3 was the standard. Local inference (for me) meant serious hardware and output that wasâ€¦ cute, but not dependable.</p>
<p>Four years later, the landscape has inverted:</p>
<p><strong>2022</strong>: Local AI was an experiment. Serious hardware, inconsistent results.
<strong>2024</strong>: Local AI became viable. A "normal" computer could run something useful.
<strong>2026</strong>: Local AI is increasingly the default. It runs on laptops. The output is genuinely good for daily use.</p>
<p>The shift was both technical and philosophical. Technical because the models got dramatically better at handling tools, long contexts, and real workflowsâ€”especially with a good harness. Philosophical because the question changed from "can I run this locally?" to:</p>
<p><strong>"Why would I let someone else control the AI I or my loved ones depend on?"</strong></p>
<hr />
<h2>Why Local-First Matters Now</h2>
<p>A lot of "AI privacy" talk stays abstract until it hits family life.</p>
<p>When you or a loved one ask an AI for helpâ€”learning support, creative projects, tough questionsâ€”that interaction is usually processed on someone else's servers in the cloud. Depending on the provider and plan, prompts may be logged for safety/abuse prevention, debugging, and product improvement. Policies differ, retention windows differ, opt-outs differâ€”but the core reality is the same:</p>
<p><strong>cloud-first puts your most human questions inside someone else's systems.</strong></p>
<p>For families, it gets real fast.</p>
<p>When a kid asks awkward questions about growing up, or someone needs a patient, judgment-free conversation during a hard weekâ€¦ I want those moments to be handled like a private conversation, not a product event.</p>
<p>So I went to work building something different.</p>
<p><strong>Cloud AI often means:</strong></p>
<ul>
<li>Prompts processed remotely (internet dependency)</li>
<li>Logs retained for some period (policies vary)</li>
<li>Provider-defined content rules and defaults</li>
<li>Provider-defined incentives (business model matters)</li>
<li>Subject to outages and account lockouts</li>
</ul>
<p><strong>Local AI means you can choose:</strong></p>
<ul>
<li>Prompts stay on your hardware (or your home server)</li>
<li>Your data doesn't have to train anything</li>
<li>Your policies, your values, your boundaries</li>
<li>Works offline (or works "home-first" over a private tunnel)</li>
<li>More control over availability and failure modes</li>
</ul>
<p>That word <strong>choose</strong> is the point.</p>
<hr />
<h2>What I Actually Built: Guardian Protocol</h2>
<p>Guardian Protocol started as a question:</p>
<p><strong>What if "parental controls" were about wellbeing and skill-buildingâ€”without a surveillance model?</strong></p>
<p>Most parental monitoring software is built on surveillance: caregivers see everything, kids see nothing. That creates an adversarial relationship. Kids learn to hide rather than learn.</p>
<p>I want to flip it.</p>
<p><strong>Transparency over surveillance.</strong> Kids see the same dashboard guardians do.
<strong>Conversation over punishment.</strong> Each block is a conversation opportunity, not a gotcha.
<strong>Digital development over screen-time obsession.</strong></p>
<p>The architecture mirrors the harness patterns we see in bigger LLM stacks:</p>
<pre><code class="language-text">Guardian Code (CLI)     = CLI equivalent
Guardian Desktop (App)  = Desktop equivalent
Guardian.ai (Web)       = Web.ai equivalent
Guardian Network        = Router/Pi running local AI
</code></pre>
<p>The differentiator is incentives: Guardian is built for family wellbeing, not ad targeting.</p>
<p>And the key capability is <em>local-first routing</em>: every AI query from any family device can route through <strong>your home server</strong>.</p>
<p>Kid at school asks a question â†’ it can tunnel back to your home AI â†’ responds with your family's boundaries and values embedded.</p>
<p>That's not hypothetical. That's exactly what mesh networking is for.</p>
<hr />
<h2>The 2026 Stack: What Actually Works</h2>
<p>After four years of experimentation, here's what I run daily.</p>
<h3>The Foundation: Many Good Options</h3>
<p>Starting out, <strong>Ollama</strong> makes local AI accessible in the simplest possible way.</p>
<p>On Linux, the official install is literally a one-liner: (<a href="https://docs.ollama.com/linux" title="Linux">Ollama Docs</a>)</p>
<pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
<p>Then pull and run a model:</p>
<pre><code class="language-bash">ollama run qwen3:14b
</code></pre>
<p>That's it. You're running a local model. (And yes, Ollama supports macOS/Windows tooâ€”but the key point is: setup is now minutes, not days.) (<a href="https://docs.ollama.com/linux" title="Linux">Ollama Docs</a>)</p>
<h3>llama.cpp: Production-Grade Control</h3>
<p>When I need more controlâ€”more tuning, more deployment options, more harness workâ€”<strong>llama.cpp</strong> is my go-to.</p>
<p>Its stated goal is straight-up what local-first builders care about: minimal setup, strong performance, wide hardware support, and support for many quantization levels to reduce memory use. (<a href="https://github.com/ggml-org/llama.cpp" title="GitHub - ggml-org/llama.cpp: LLM inference in C/C++">GitHub</a>)</p>
<p>In practice, this is where "local AI as infrastructure" becomes real: repeatable installs, reproducible model formats (GGUF), and a stack you can ship as a service inside your home or your org.</p>
<hr />
<h2>Models Worth Running (January 2026)</h2>
<p>I'm not interested in model fandom. I'm interested in: <em>what runs well, what's licensable, what's teachable, and what families can actually own.</em></p>
<h3>Qwen3: My default open-weight family right now</h3>
<p>The Qwen team explicitly "open-weighted" <strong>six dense models</strong> (0.6B, 1.7B, 4B, 8B, 14B, 32B) plus two MoE models (including <strong>Qwen3-30B-A3B</strong> and <strong>Qwen3-235B-A22B</strong>) under the <strong>Apache 2.0 license</strong>. (<a href="https://qwenlm.github.io/blog/qwen3/" title="Qwen3: Think Deeper, Act Faster">Qwen</a>)</p>
<p>They also report a real "parameter efficiency" jump: Qwen3 base models performing comparably to larger Qwen2.5 base models (e.g., 1.7B â‰ˆ 3B; 4B â‰ˆ 7B; 8B â‰ˆ 14B; 14B â‰ˆ 32B; 32B â‰ˆ 72B). That's not me hypingâ€”it's their published claim, and it tracks with what I see in day-to-day work: you get a lot more usefulness per gigabyte than you used to. (<a href="https://qwenlm.github.io/blog/qwen3/" title="Qwen3: Think Deeper, Act Faster">Qwen</a>)</p>
<h3>What I recommend people actually run</h3>
<p>Instead of arguing theoretical "best," I like tables you can reproduce.</p>
<p>These are <strong>Ollama's listed download sizes and context windows</strong> for the Qwen3 tags (so readers can verify quickly). (<a href="https://ollama.com/library/qwen3" title="qwen3">Ollama</a>)</p>
<table>
<thead>
<tr>
<th>Model</th>
<th style="text-align: right;">Approx download size</th>
<th style="text-align: right;">Context window shown in Ollama</th>
<th>Good for</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Qwen3 0.6B</strong></td>
<td style="text-align: right;">~523MB</td>
<td style="text-align: right;">40K</td>
<td>Edge devices, Raspberry Pi demos, "runs anywhere" helpers</td>
</tr>
<tr>
<td><strong>Qwen3 1.7B</strong></td>
<td style="text-align: right;">~1.4GB</td>
<td style="text-align: right;">40K</td>
<td>Quick queries, learning support, lightweight workflows</td>
</tr>
<tr>
<td><strong>Qwen3 4B</strong></td>
<td style="text-align: right;">~2.5GB</td>
<td style="text-align: right;">256K</td>
<td>Daily chat, drafting, surprisingly good long-context work</td>
</tr>
<tr>
<td><strong>Qwen3 8B</strong></td>
<td style="text-align: right;">~5.2GB</td>
<td style="text-align: right;">40K</td>
<td>Daily driver for most people (coding + general use)</td>
</tr>
<tr>
<td><strong>Qwen3 14B</strong></td>
<td style="text-align: right;">~9.3GB</td>
<td style="text-align: right;">40K</td>
<td>Heavier reasoning + synthesis + research support</td>
</tr>
<tr>
<td><strong>Qwen3 32B</strong></td>
<td style="text-align: right;">~20GB</td>
<td style="text-align: right;">40K</td>
<td>Higher ceiling work if you have the RAM/VRAM</td>
</tr>
<tr>
<td><strong>Qwen3 30B (MoE)</strong></td>
<td style="text-align: right;">~19GB</td>
<td style="text-align: right;">256K</td>
<td>MoE efficiency (big feel, different cost profile)</td>
</tr>
</tbody>
</table>
<p><strong>My daily drivers:</strong> Qwen3 8B and 14B.</p>
<h3>Deep reasoning: DeepSeek-R1</h3>
<p>When I want "slow thinking" for research and hard reasoning, I reach for <strong>DeepSeek-R1</strong> and distill variants.</p>
<p>DeepSeek's own release notes frame R1 as "performance on par with OpenAI-o1," and they state that <strong>code and models are MIT-licensed</strong>, explicitly encouraging distillation and commercial use. (<a href="https://api-docs.deepseek.com/news/news250120" title="DeepSeek-R1 Release">DeepSeek API Docs</a>)</p>
<p>Their GitHub repo also states plainly that DeepSeek-R1 supports commercial use and derivative works including distillation for training other LLMs. (<a href="https://github.com/deepseek-ai/DeepSeek-R1" title="DeepSeek-R1">GitHub</a>)</p>
<hr />
<h2>LocalScore: Measure Reality, Not Vibes</h2>
<p>Benchmarks can be silly. But <strong>LocalScore</strong> is one of the rare benchmarks that's actually aligned with the user experience.</p>
<p>LocalScore combines:</p>
<ul>
<li>prompt processing speed</li>
<li>generation speed</li>
<li>time-to-first-token</li>
</ul>
<p>â€¦and turns that into a single comparable score. (<a href="https://www.localscore.ai/about" title="About LocalScore">LocalScore</a>)</p>
<p>Their interpretation is also refreshingly practical:</p>
<ul>
<li><strong>1,000 is excellent</strong></li>
<li><strong>250 is passable</strong></li>
<li><strong>below 100 is likely a poor experience</strong> (<a href="https://www.localscore.ai/about" title="About LocalScore">LocalScore</a>)</li>
</ul>
<p>LocalScore is labeled as <strong>"A Mozilla Builders Project"</strong> on the site and accelerator pages, and it notes that it leverages <strong>llamafile</strong> under the hood for portability. (<a href="https://www.localscore.ai/accelerator/1704" title="NVIDIA GeForce RTX 4090 Results">LocalScore</a>)</p>
<p>If you're teaching families, running workshops, or building anything you want to be repeatable, this matters: you can stop arguing and start measuring.</p>
<hr />
<h2>Real Hardware, Real Numbers</h2>
<p>Instead of "trust me," here are <strong>public LocalScore submissions</strong> you can click and verify.</p>
<h3>Example: RTX 4090</h3>
<p>On a published RTX 4090 entry (23GB), LocalScore reports:</p>
<ul>
<li><strong>~1727</strong> on the 8B-class test</li>
<li><strong>~972</strong> on the 14B-class test (<a href="https://www.localscore.ai/accelerator/1704" title="NVIDIA GeForce RTX 4090 Results">LocalScore</a>)</li>
</ul>
<p>A different RTX 4090 entry (24GB) shows:</p>
<ul>
<li><strong>~1421</strong> (8B-class)</li>
<li><strong>~714</strong> (14B-class) (<a href="https://www.localscore.ai/accelerator/77" title="NVIDIA GeForce RTX 4090 Results">LocalScore</a>)</li>
</ul>
<p>That variance is the point: settings and configs matter, but the order-of-magnitude story is stable.</p>
<h3>Example: Mac Studio M3 Ultra (256GB)</h3>
<p>A published Apple M3 Ultra entry (256GB) reports:</p>
<ul>
<li><strong>~394</strong> on the 8B-class test</li>
<li><strong>~216</strong> on the 14B-class test (<a href="https://www.localscore.ai/accelerator/1359" title="Apple M3 Ultra 24P+8E+80GPU Results">LocalScore</a>)</li>
</ul>
<p>This is why I like LocalScore: it forces honesty. Some machines are monsters at certain classes of workloads, and merely "fine" at others.</p>
<h3>What I tell people now</h3>
<ul>
<li>If you want local AI to feel "normal," aim for <strong>~250+ on the 8B test</strong> on <em>your hardware</em>, with your runtime. (<a href="https://www.localscore.ai/about" title="About LocalScore">LocalScore</a>)</li>
<li>If you're building for a household or workshop (multiple users), you want more headroom than "passable."</li>
</ul>
<hr />
<h2>Licensing: Ownership Is a Feature</h2>
<p>This is where a lot of local-first writing gets hand-wavy. I'm not willing to be hand-wavy here.</p>
<h3>Qwen3: Apache 2.0</h3>
<p>Qwen3 is published as Apache 2.0 in the official release. (<a href="https://qwenlm.github.io/blog/qwen3/" title="Qwen3: Think Deeper, Act Faster">Qwen</a>)</p>
<p>Apache 2.0 is permissive and widely used, but it does have real obligations around including the license text and handling notices. (Normal, manageable stuff.) (<a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache License, Version 2.0">Apache Software Foundation</a>)</p>
<h3>DeepSeek-R1: MIT</h3>
<p>DeepSeek-R1 states that the code and weights are MIT-licensed, and explicitly permits commercial use and derivative works including distillation. (<a href="https://github.com/deepseek-ai/DeepSeek-R1" title="DeepSeek-R1">GitHub</a>)</p>
<p>MIT is also permissive, with minimal conditions (generally: keep the copyright + license text). (<a href="https://opensource.org/license/mit" title="The MIT License">Open Source Initiative</a>)</p>
<h3>Why I moved away from Meta's Llama for "build it and own it forever"</h3>
<p>Meta's Llama models are strong. This is not a model-quality complaint.</p>
<p>It's a licensing and downstream simplicity complaint.</p>
<p>For example, the Llama 3.1 Community License (as published on the model card) includes redistribution requirements like:</p>
<ul>
<li>include the agreement when you redistribute</li>
<li><strong>prominently display "Built with Llama"</strong> in certain product/documentation contexts</li>
<li>include "Llama" at the beginning of a model name if you use Llama materials/outputs to train a distributed model</li>
<li>additional commercial terms tied to a <strong>700 million monthly active user</strong> threshold (<a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct" title="meta-llama/Llama-3.1-8B-Instruct Â· Hugging Face">Hugging Face</a>)</li>
</ul>
<p>Separately, the Open Source Initiative has publicly argued that Meta's Llama licensing does <strong>not</strong> meet the Open Source Definition (they frame it as "open washing"). (<a href="https://opensource.org/blog/metas-llama-license-is-still-not-open-source" title="Meta's LLaMa license is still not Open Source â€“ Open Source Initiative">Open Source Initiative</a>)</p>
<p>None of this means "never use Llama." It means: if you're teaching families and community orgs to build tools they can own forever, permissive licensing is not an ideological flexâ€”it's a durability strategy.</p>
<hr />
<h2>The Interface Layer</h2>
<p>For my work, I'm primarily:</p>
<ul>
<li>in the terminal with llama.cpp</li>
<li>building harnesses with FastAPI + HTMX</li>
<li>deploying specialized setups for educators (e.g., local-first access patterns that avoid shipping student data to third parties)</li>
</ul>
<p>For family setups and workshops, visual interfaces matter. Beginners need something they can click before they can love the terminal.</p>
<hr />
<h2>The Family AI Architecture: Mobile as Mesh Node</h2>
<p>Here's the Guardian Protocol "full picture" insight:</p>
<p>your phone isn't just a deviceâ€”it's a <strong>node</strong>.</p>
<pre><code class="language-text">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      YOUR HOME                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  llama.cpp      â”‚    â”‚  Guardian       â”‚             â”‚
â”‚  â”‚  - Local LLMs   â”‚â—„â”€â”€â–¶â”‚  - Consent layerâ”‚             â”‚
â”‚  â”‚  - Your control â”‚    â”‚  - Family valuesâ”‚             â”‚
â”‚  â”‚                 â”‚    â”‚  - Transparency â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                  â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                    â”‚                    â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚Kid's phoneâ”‚        â”‚Kid's tabletâ”‚       â”‚Kid's laptopâ”‚
        â”‚ at school â”‚        â”‚at friend's â”‚       â”‚at library â”‚
        â”‚ (MESH NODE)â”‚       â”‚(MESH NODE) â”‚       â”‚(MESH NODE) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        ALL DEVICES ARE MESH NODES
        CONNECTING BACK TO HOME AI
        FAMILY AI, FAMILY VALUES, ANYWHERE
</code></pre>
<p>Tools like <strong>Headscale</strong> (an open source, self-hosted implementation of the Tailscale control server) exist specifically to exchange WireGuard keys and coordinate private networks without requiring a cloud control plane. (<a href="https://github.com/juanfont/headscale" title="juanfont/headscale: An open source, self-hosted ...">GitHub</a>)</p>
<p>And if you want a practical walkthrough of "local AI + secure remote access," Tailscale has published a step-by-step guide showing an offline AI lab wired up with Ollama + secure access. (<a href="https://tailscale.com/blog/self-host-a-local-ai-stack" title="Self-host a local AI stack and access it from anywhere">Tailscale</a>)</p>
<p>This is what makes local AI portable: not "run a model on your phone," but "bring your home AI with you."</p>
<hr />
<h2>Teaching Families to Build</h2>
<p>Running local AI for my own family was step one.</p>
<p>Step two is teaching the <strong>producer mindset</strong>: families building tools, not just subscribing to someone else's interface.</p>
<p>The success metric I care about is simple:
<strong>every participant builds 2â€“3 working AI tools they own forever.</strong></p>
<p>This only works economically because local AI works. If I had to provision cloud credits for every family, the economics would collapse.</p>
<hr />
<h2>What Changed My Mind</h2>
<p>In 2022, I thought local AI was a hobby. Something for tinkerers with more time than sense.</p>
<p>In 2026, I see it as infrastructureâ€”like having a backup power source or a water filter.</p>
<p><strong>Quality rose.</strong> Open-weight models got good enough to be trusted for real tasks.
<strong>Tooling matured.</strong> What used to take days now takes minutes.
<strong>Licensing became central.</strong> If you can't legally own what you build, you don't really own it.
<strong>Privacy stakes rose.</strong> AI is becoming part of daily life. Families deserve control.</p>
<p>And yes: the Axios prediction energy is realâ€”2026 is being framed as the year AI has to prove ROI ("show me the money"). For families and communities, ROI isn't measured in dollars. It's measured in ownership, privacy, and capability. (<a href="https://www.axios.com/2026/01/01/ai-2026-money-openai-google-anthropic-agents" title="2026 is AI's \&quot;show me the money\&quot; year">Axios</a>)</p>
<hr />
<h2>Getting Started Today</h2>
<h3>Minimum Viable Setup (fast)</h3>
<ol>
<li>Install Ollama (Linux): (<a href="https://docs.ollama.com/linux" title="Linux">Ollama Docs</a>)</li>
</ol>
<p><code>bash
   curl -fsSL https://ollama.com/install.sh | sh</code>
2. Run a model:</p>
<p><code>bash
   ollama run qwen3:8b</code></p>
<p>No account. No API key. You're local.</p>
<h3>Family Setup (home-first)</h3>
<ul>
<li>Put Ollama or llama.cpp on one "home" machine</li>
<li>Add a simple UI</li>
<li>Create shared prompts and guardrails</li>
<li>Then add mesh access so devices can tunnel home</li>
</ul>
<hr />
<h2>The Tools I've Tried (And Verdicts)</h2>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Verdict</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>llama.cpp</strong></td>
<td>Essential</td>
<td>Local-first inference with strong hardware support and quantization options (<a href="https://github.com/ggml-org/llama.cpp" title="GitHub - ggml-org/llama.cpp: LLM inference in C/C++">GitHub</a>)</td>
</tr>
<tr>
<td><strong>Ollama</strong></td>
<td>Essential for starting</td>
<td>Fast install + model library; easiest on-ramp (<a href="https://docs.ollama.com/linux" title="Linux">Ollama Docs</a>)</td>
</tr>
<tr>
<td><strong>LocalScore</strong></td>
<td>Essential for benchmarking</td>
<td>Measures what users feel; Mozilla Builders Project (<a href="https://www.localscore.ai/about" title="About LocalScore">LocalScore</a>)</td>
</tr>
<tr>
<td><strong>Tailscale/Headscale</strong></td>
<td>Essential for mesh</td>
<td>"Home AI everywhere" becomes real (<a href="https://github.com/juanfont/headscale" title="juanfont/headscale: An open source, self-hosted ...">GitHub</a>)</td>
</tr>
<tr>
<td><strong>vLLM/SGLang</strong></td>
<td>Production servers</td>
<td>Great when you're serving many users (bigger org deployments)</td>
</tr>
<tr>
<td><strong>LM Studio</strong></td>
<td>Good for beginners</td>
<td>Native UI is a huge unlock for families</td>
</tr>
<tr>
<td><strong>Jan</strong></td>
<td>Promising</td>
<td>Privacy-first vibes, still evolving</td>
</tr>
<tr>
<td>GPT4All</td>
<td>Dated (for my use)</td>
<td>Ollama + llama.cpp became my defaults</td>
</tr>
<tr>
<td>Text Generation WebUI</td>
<td>Too complex (for families)</td>
<td>Powerful, but steep for workshops</td>
</tr>
</tbody>
</table>
<hr />
<h2>What's Next</h2>
<p>Local AI is now "good enough" for most daily work. And "good enough" is getting better.</p>
<p><strong>What I'm watching:</strong></p>
<ul>
<li>Smaller models with better reasoning (and better post-training)</li>
<li>Speculative decoding and other throughput boosts</li>
<li>MoE architectures where only a fraction of parameters are active per token (Qwen3 explicitly ships MoE variants) (<a href="https://qwenlm.github.io/blog/qwen3/" title="Qwen3: Think Deeper, Act Faster">Qwen</a>)</li>
<li>Better on-device inference across Apple Silicon and new PC chips</li>
<li>Better family-first tooling (consent layers, transparency dashboards, shared norms)</li>
</ul>
<p><strong>What I'm building:</strong></p>
<ul>
<li>Guardian Protocol (consent-based AI for families)</li>
<li>Curriculum that makes families producers first</li>
<li>Local-first educational deployments that keep data sovereign</li>
</ul>
<p>Four years ago, running local AI took real commitment. Today, the tools are ready, the models are good, and the stakes are higher.</p>
<p>The technology exists. The only question is whether you'll use it.</p>
<p><strong>Let's GrOw!</strong></p>
<hr />
<h2>Resources</h2>
<p>(Names + proof links you can cite in a blog.)</p>
<ul>
<li><strong>LocalScore</strong> (what it measures + how to interpret scores) (<a href="https://www.localscore.ai/about" title="About LocalScore">LocalScore</a>)</li>
<li><strong>Qwen3 official release</strong> (model lineup + Apache 2.0 + base-model comparison claims) (<a href="https://qwenlm.github.io/blog/qwen3/" title="Qwen3: Think Deeper, Act Faster">Qwen</a>)</li>
<li><strong>Ollama Linux install</strong> (official one-liner) (<a href="https://docs.ollama.com/linux" title="Linux">Ollama Docs</a>)</li>
<li><strong>Ollama Qwen3 library listing</strong> (sizes + context windows shown) (<a href="https://ollama.com/library/qwen3" title="qwen3">Ollama</a>)</li>
<li><strong>llama.cpp</strong> (core repo) (<a href="https://github.com/ggml-org/llama.cpp" title="GitHub - ggml-org/llama.cpp: LLM inference in C/C++">GitHub</a>)</li>
<li><strong>Headscale</strong> (self-hosted control server; WireGuard key exchange) (<a href="https://github.com/juanfont/headscale" title="juanfont/headscale: An open source, self-hosted ...">GitHub</a>)</li>
<li><strong>Tailscale local AI stack guide</strong> (practical "self-host local AI + access anywhere") (<a href="https://tailscale.com/blog/self-host-a-local-ai-stack" title="Self-host a local AI stack and access it from anywhere">Tailscale</a>)</li>
<li><strong>DeepSeek-R1 release</strong> (MIT license; distill &amp; commercialize freely) (<a href="https://api-docs.deepseek.com/news/news250120" title="DeepSeek-R1 Release">DeepSeek API Docs</a>)</li>
<li><strong>Llama 3.1 license excerpt</strong> ("Built with Llama" + 700M MAU clause) (<a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct" title="meta-llama/Llama-3.1-8B-Instruct Â· Hugging Face">Hugging Face</a>)</li>
<li><strong>OSI critique of Llama licensing</strong> (why they say it's not Open Source) (<a href="https://opensource.org/blog/metas-llama-license-is-still-not-open-source" title="Meta's LLaMa license is still not Open Source â€“ Open Source Initiative">Open Source Initiative</a>)</li>
</ul>
<hr />]]></description>
      <pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://w4ester.github.io/wf-ai-site#four-years-of-local-ai-2026-01-13</guid>
    </item>
    <item>
      <title>Publishing Architecture: How Any AI Platform Can Post to a Static Site</title>
      <link>https://w4ester.github.io/wf-ai-site</link>
      <description><![CDATA[<p>A reference implementation for AI-assisted content publishing</p>
<p>This post documents the architecture behind wf-ai-siteâ€”a pattern that any AI platform can adopt to give users a simple, owned publishing outlet.</p>
<hr />
<h2>The Problem</h2>
<p>Most AI platforms are walled gardens. You chat, you generate, you... lose it all when you close the tab. Even with history features, your ideas stay trapped in someone else's database.</p>
<p><strong>What if every AI session could publish to a place you own?</strong></p>
<hr />
<h2>The Architecture</h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AI-ASSISTED PUBLISHING                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  AI Platform â”‚â”€â”€â”€â–ºâ”‚  CLI/API     â”‚â”€â”€â”€â–ºâ”‚ Static Site  â”‚       â”‚
â”‚  â”‚  (Ollama,    â”‚    â”‚  (manager.py)â”‚    â”‚ (GitHub      â”‚       â”‚
â”‚  â”‚   llama.cpp, â”‚    â”‚              â”‚    â”‚  Pages)      â”‚       â”‚
â”‚  â”‚   WF-AI)     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                    â”‚               â”‚
â”‚                             â–¼                    â–¼               â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                    â”‚ index.html   â”‚    â”‚  feed.xml    â”‚         â”‚
â”‚                    â”‚ (content)    â”‚    â”‚  (RSS)       â”‚         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p><strong>Components:</strong></p>
<ol>
<li><strong>AI Platform</strong> - Any conversational AI (Ollama, llama.cpp, custom agents)</li>
<li><strong>CLI Tool</strong> - <code>manager.py</code> handles post creation, RSS generation</li>
<li><strong>Static Site</strong> - Single <code>index.html</code> hosted on GitHub Pages (free, no backend)</li>
<li><strong>RSS Feed</strong> - Auto-generated for subscribers</li>
</ol>
<hr />
<h2>Integration Points</h2>
<h3>For AI Agent Platforms</h3>
<p>Create a command or skill in your agent's configuration:</p>
<pre><code class="language-yaml">---
description: Create and publish a new post
---

# Post to your site

Gather title, content, tags from user.
Run: python manager.py post &quot;TITLE&quot; &quot;CONTENT&quot; --tags TAGS
Run: python manager.py rss
Offer: git add . &amp;&amp; git commit &amp;&amp; git push
</code></pre>
<p>Now your agent can trigger AI-assisted publishing through a simple command.</p>
<h3>For Any AI Platform</h3>
<p>The pattern is simple:</p>
<pre><code class="language-python"># 1. Accept content from AI conversation
title = &quot;My Post Title&quot;
content = &quot;Markdown content here...&quot;
tags = [&quot;ai&quot;, &quot;thoughts&quot;]

# 2. Call the manager
import subprocess
subprocess.run([
    &quot;python&quot;, &quot;manager.py&quot;, &quot;post&quot;,
    title, content,
    &quot;--tags&quot;, &quot;,&quot;.join(tags)
])

# 3. Update RSS
subprocess.run([&quot;python&quot;, &quot;manager.py&quot;, &quot;rss&quot;])

# 4. Deploy (optional - could be automated)
subprocess.run([&quot;git&quot;, &quot;add&quot;, &quot;.&quot;])
subprocess.run([&quot;git&quot;, &quot;commit&quot;, &quot;-m&quot;, f&quot;New post: {title}&quot;])
subprocess.run([&quot;git&quot;, &quot;push&quot;])
</code></pre>
<h3>For MCP Integration</h3>
<p>Expose as an MCP tool:</p>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;publish_post&quot;,
  &quot;description&quot;: &quot;Publish content to user's static site&quot;,
  &quot;parameters&quot;: {
    &quot;title&quot;: { &quot;type&quot;: &quot;string&quot; },
    &quot;content&quot;: { &quot;type&quot;: &quot;string&quot; },
    &quot;tags&quot;: { &quot;type&quot;: &quot;array&quot; }
  }
}
</code></pre>
<p>Any MCP-compatible AI can now publish.</p>
<hr />
<h2>Why Static?</h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Static Site</th>
<th>CMS/Database</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cost</td>
<td>Free (GitHub Pages)</td>
<td>$5-50/month</td>
</tr>
<tr>
<td>Complexity</td>
<td>One HTML file</td>
<td>Server, DB, auth</td>
</tr>
<tr>
<td>Ownership</td>
<td>Git repo you control</td>
<td>Platform lock-in</td>
</tr>
<tr>
<td>Speed</td>
<td>Instant (CDN)</td>
<td>Variable</td>
</tr>
<tr>
<td>Security</td>
<td>No attack surface</td>
<td>Constant patching</td>
</tr>
<tr>
<td>Portability</td>
<td>Copy files anywhere</td>
<td>Migration pain</td>
</tr>
</tbody>
</table>
<p><strong>The constraint is the feature.</strong> A single HTML file forces simplicity.</p>
<hr />
<h2>The manager.py Contract</h2>
<p>Any AI platform can integrate if it can call these commands:</p>
<pre><code class="language-bash"># Create post (returns success/failure)
python manager.py post &quot;Title&quot; &quot;Content&quot; --tags tag1,tag2

# Regenerate RSS feed
python manager.py rss

# List recent posts
python manager.py list
</code></pre>
<p>That's it. Three commands. Universal integration.</p>
<hr />
<h2>Browser Automation Bonus</h2>
<p>For AI platforms with browser access (Playwright, Puppeteer, etc.):</p>
<pre><code class="language-python"># Verify published content
browser.navigate(&quot;https://yoursite.github.io&quot;)
snapshot = browser.snapshot()
# AI can now confirm post appears correctly
</code></pre>
<p>The AI can verify its own publishing worked.</p>
<hr />
<h2>Extending the Pattern</h2>
<p>This architecture supports:</p>
<ul>
<li><strong>Multiple sites</strong> - Different manager.py configs for different audiences</li>
<li><strong>Scheduling</strong> - BEADS integration for post queues</li>
<li><strong>Review workflows</strong> - AI drafts, human approves, then publishes</li>
<li><strong>Cross-posting</strong> - One source, publish to multiple platforms</li>
<li><strong>Analytics</strong> - RSS subscriber counts, no invasive tracking</li>
</ul>
<hr />
<h2>Get Started</h2>
<ol>
<li>Fork <a href="https://github.com/w4ester/wf-ai-site">wf-ai-site</a></li>
<li>Enable GitHub Pages on your fork</li>
<li>Add a publishing command to your AI agent</li>
<li>Start publishing</li>
</ol>
<p>Your ideas deserve a space you own.</p>
<hr />
<p><em>This post was created using the architecture it describes.</em> ğŸ”„</p>]]></description>
      <pubDate>Mon, 29 Dec 2025 16:55:39 +0000</pubDate>
      <guid>https://w4ester.github.io/wf-ai-site#publishing-architecture:-how-any-ai-platform-can-post-to-a-static-site</guid>
    </item>
    <item>
      <title>The Family Safety Protocol: What If We Built Digital Wellbeing Like We Built Language Servers?</title>
      <link>https://w4ester.github.io/wf-ai-site</link>
      <description><![CDATA[<p>A vision for family-centered technology that teaches instead of surveils</p>
<p><strong>Project:</strong> <a href="https://github.com/w4ester/WF-AI-Platform">WF-AI-Platform</a><br />
<strong>Issue:</strong> WF-AI-Platform-nl9<br />
<strong>Status:</strong> Open Epic (P2)</p>
<hr />
<h2>The Problem Nobody Talks About</h2>
<p>Every parenting app on the market shares the same DNA: surveillance capitalism dressed up as safety.</p>
<p>Circle. Bark. Qustodio. Net Nanny. They all promise to "protect your children" but what they deliver is a panopticon with a friendly UI. Parents/guardians spy on children. Children learn to evade. Trust erodes. The only winner is the company harvesting your family's data.</p>
<p>We've accepted this as normal. It isn't.</p>
<hr />
<h2>A Better Analogy: What LSP Did for Editors</h2>
<p>In the early days of programming, every text editor had to reinvent language support. Vim had its own Python integration. Emacs had its own. VS Code had its own. Sublime had its own. It was wasteful, inconsistent, and held back innovation.</p>
<p>Then Microsoft created the Language Server Protocol (LSP). One protocol. Any editor. Any language.</p>
<p>Suddenly, a single Python language server could power Vim, Emacs, VS Code, and everything else. The ecosystem exploded. Quality went up. Duplication went down.</p>
<p><strong>What if we did the same thing for family digital safety?</strong></p>
<hr />
<h2>Introducing FSP: The Family Safety Protocol</h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FAMILY WELLBEING PLATFORM                     â”‚
â”‚         &quot;Digital tools that help humans grow up healthy&quot;         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  AI Agents (wf-ai) â—„â”€â”€â–º FSP Protocol â—„â”€â”€â–º Family Dashboard      â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ FSP Implementations: OpenWrt, ASUS, Gryphon, Pi-hole    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  LOCAL-FIRST: All data stays on family hardware                  â”‚
â”‚  OPEN: Protocol is open, anyone can implement                    â”‚
â”‚  TRANSPARENT: Children can see what parents/guardians see                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>FSP isn't a product. It's a protocol. Like LSP, it defines how components talk to each other:</p>
<ul>
<li>Router plugins (OpenWrt, ASUS, Pi-hole) implement FSP</li>
<li>Dashboards consume FSP data</li>
<li>AI agents configure and explain using FSP</li>
<li>Families own everythingâ€”the hardware, the data, the rules</li>
</ul>
<p><strong>One protocol. Any router. Any dashboard. Any AI assistant.</strong></p>
<hr />
<h2>The Philosophy: Wellbeing, Not Surveillance</h2>
<p>This isn't a rebrand. It's a fundamental shift in values.</p>
<table>
<thead>
<tr>
<th>Surveillance Model</th>
<th>Wellbeing Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Parents/guardians spy on children</td>
<td>Families learn together</td>
</tr>
<tr>
<td>Block and control</td>
<td>Understand and grow</td>
</tr>
<tr>
<td>Opaque rules</td>
<td>Transparent agreements</td>
</tr>
<tr>
<td>"Caught you!"</td>
<td>"Let's talk about this"</td>
</tr>
<tr>
<td>Corporate data extraction</td>
<td>Local-first, family-owned</td>
</tr>
<tr>
<td>Consumer mindset</td>
<td>Producer mindset</td>
</tr>
</tbody>
</table>
<p>The last row matters most. Today's parenting apps treat families as <em>consumers</em> of safety. FSP treats families as <em>producers</em> of their own digital culture.</p>
<hr />
<h2>Three Pillars</h2>
<h3>1. Transparency Over Surveillance</h3>
<p>Children and young people see the same dashboard parents/guardians see. There are no secret reports. No hidden tracking. No gotcha moments.</p>
<p><strong>Why?</strong> Because surveillance destroys trust. And trust is the actual thing that keeps children safeâ€”not filters.</p>
<p>When a child knows exactly what data exists and who can see it, they learn to make informed decisions. When parents/guardians model transparency, they teach integrity.</p>
<h3>2. Teaching Over Blocking</h3>
<p>Every block is a conversation starter.</p>
<p><strong>Current tools:</strong> "This site is blocked." End of story.</p>
<p><strong>FSP approach:</strong> "This site was filtered because it contains content we agreed wasn't appropriate yet. Here's why. Here are some questions to think about. Want to talk about it at dinner?"</p>
<p>The AI generates age-appropriate explanations. The goal isn't to hide the internetâ€”it's to develop the judgment to navigate it.</p>
<h3>3. Growth Over Control</h3>
<p>Screen time is a garbage metric. It tells you nothing about what a child learned, created, or experienced.</p>
<p>FSP tracks <em>development</em>:<br />
- What skills are emerging?<br />
- What curiosities are they following?<br />
- What creative work are they producing?<br />
- What digital literacy milestones have they reached?</p>
<p><strong>Celebrate progress. Don't just count minutes.</strong></p>
<hr />
<h2>What the AI Does</h2>
<p>This is where WF-AI-Platform comes in.</p>
<p>Local AI agents (running on family hardware via llama.cpp) provide:</p>
<p><strong>Configuration in Plain English:</strong><br />
"Block social media during homework time but allow Khan Academy and Wikipedia"</p>
<p>The AI translates this to router rules via FSP.</p>
<p><strong>Conversation Starters:</strong></p>
<pre><code class="language-typescript">interface FamilyConversation {
  topic: &quot;YouTube Usage&quot;;
  context: &quot;Emma spent 2 hours watching science videos today&quot;;
  questions: [
    &quot;What did you discover?&quot;,
    &quot;Was there anything surprising?&quot;,
    &quot;Want to try that experiment together?&quot;
  ];
  resources: [/* related books, activities, videos */];
}
</code></pre>
<p><strong>Digital Literacy Curriculum:</strong><br />
Personalized learning paths based on age, interests, and family values. Not one-size-fits-all fear mongering.</p>
<p><strong>Family Agreement Drafting:</strong><br />
AI helps families write technology agreements together. Not rules imposed by parents/guardiansâ€”agreements negotiated as a family.</p>
<hr />
<h2>The Technical Shape</h2>
<pre><code class="language-typescript">interface WellbeingProfile {
  name: string;                  // &quot;Emma's Learning Profile&quot;
  dailyScreenGoals: Duration;    // Goals, not limits
  focusTime: Schedule;           // Homework, creative time
  restTime: Schedule;            // Wind-down, sleep
  contentGuidance: ContentLevel; // Age-appropriate guidance
  transparencyLevel: 'full' | 'summary' | 'private';
}
</code></pre>
<p>Notice the language:<br />
- <strong>Goals</strong>, not limits<br />
- <strong>Guidance</strong>, not restrictions<br />
- <strong>Transparency levels</strong> that respect growing autonomy</p>
<p>A 7-year-old might have <code>transparencyLevel: 'full'</code>â€”parents/guardians see everything, and so does the child.</p>
<p>A 16-year-old might negotiate <code>transparencyLevel: 'summary'</code>â€”parents/guardians see aggregate patterns, not individual sites. Trust, verified.</p>
<hr />
<h2>Open Questions</h2>
<p>This is a vision, not a finished product. Hard questions remain:</p>
<ol>
<li><strong>Scope:</strong> Start with Baltimore AI Producers Lab? Or design for scale from day one?</li>
<li><strong>Hardware:</strong> Should there be a "reference device" families can build, buy, or access through community lending?</li>
<li><strong>AI:</strong> Local-only (llama.cpp) for privacy? Or hybrid with cloud for complex tasks?</li>
<li><strong>Sustainability:</strong> Open source with services? Grant-funded? Community-supported?</li>
<li><strong>Governance:</strong> How do families participate in protocol development?</li>
<li><strong>Privacy Verification:</strong> How do we prove data stays local?</li>
<li><strong>Partnerships:</strong> Schools? Pediatricians? Family therapists?</li>
</ol>
<hr />
<h2>Possible Names</h2>
<p>The protocol needs a name that captures its spirit:</p>
<ul>
<li><strong>Guardian Protocol</strong> â€” protective but not controlling</li>
<li><strong>Family Wellbeing Protocol (FWP)</strong> â€” clinical but clear</li>
<li><strong>Digital Hearth</strong> â€” home, warmth, gathering</li>
<li><strong>Lighthouse</strong> â€” guidance through darkness</li>
<li><strong>Campfire</strong> â€” families gathering, sharing, learning</li>
</ul>
<hr />
<h2>The Heart of It</h2>
<p>This project grew out of building WF-AI-Platformâ€”a local-first AI platform where humans use AI, not the other way around.</p>
<p>The same philosophy applies to families:</p>
<blockquote>
<p><strong>"Digital tools that help humans grow up healthy."</strong></p>
</blockquote>
<p>Not tools that spy on children for parents/guardians.<br />
Not tools that extract data for corporations.<br />
Not tools that replace caregiving with algorithms.</p>
<p>Tools that support families in raising humans who can navigate the digital world with wisdom, judgment, and integrity.</p>
<p><strong>That's the vision. Now let's build it.</strong></p>
<hr />
<p><em>Let's GrOw.</em> ğŸŒ±</p>
<hr />
<p><em>This post documents an open epic in the WF-AI-Platform project. Collaboration welcome.</em></p>]]></description>
      <pubDate>Mon, 29 Dec 2025 16:35:42 +0000</pubDate>
      <guid>https://w4ester.github.io/wf-ai-site#the-family-safety-protocol:-what-if-we-built-digital-wellbeing-like-we-built-language-servers?</guid>
    </item>
    <item>
      <title>Guardian Protocol: Family Digital Wellbeing</title>
      <link>https://w4ester.github.io/wf-ai-site</link>
      <description><![CDATA[<p>Building tools for families to <strong>own their digital lives</strong>, not rent them.</p>
<p>Guardian Protocol is our approach to family-centered AI that keeps data local, respects consent, and puts parents/guardians in control without surveillance capitalism.</p>
<p>Core principles:<br />
- <strong>Local-first</strong>: Your family's data stays on your devices<br />
- <strong>Consent-based</strong>: Kids learn digital autonomy, not just restriction<br />
- <strong>Transparent</strong>: No black-box algorithms deciding what your kids see<br />
- <strong>Sovereign</strong>: Export everything, delete everything, own everything</p>
<p>Part of the Baltimore AI Producers Lab mission: teaching families to be AI <em>producers</em>, not just consumers.</p>
<p>More coming soon. ğŸ›¡ï¸</p>]]></description>
      <pubDate>Mon, 22 Dec 2025 15:01:12 +0000</pubDate>
      <guid>https://w4ester.github.io/wf-ai-site#guardian-protocol:-family-digital-wellbeing</guid>
    </item>
  </channel>
</rss>